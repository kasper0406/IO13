\documentclass[a4paper,12pt]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{mathpazo}
\usepackage{wasysym}
\usepackage{graphicx}
\usepackage{tikz,pgfplots}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{natbib} 
\usepackage{geometry}
\usetikzlibrary{fit,shapes.misc,snakes}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}

\newcommand{\ord}{\operatorname{ord}}

\begin{document}

\title{IO\\Project 1}

\author{Lasse Espeholt - 20093223\\
Kasper Nielsen - 20091182\\
Andreas Kristiansen - 20092027\\}

\maketitle
\begin{figure}[h!]
\includegraphics[width=\textwidth]{"images/forside"}
\end{figure}


\vfill{}
\begin{description}
\item [{Implementation~code~and~test~results:}]
\texttt{http://github.com/kasper0406/IO13/}
\end{description}
\pagebreak{}\tableofcontents{}\pagebreak{}

- Memory compressing
- SSd
- caching (memory

\section{Introduction}
\input{introduction.tex}

\section{Setup}
This section presents the test setup, how measurements are performed
and gives an overview of the files attached to this report.

\subsection{Test setup}
All measurements presented in this report was performed on a computer
with an Intel \todo{Insert CPU model} CPU with the following technical
specifications:
\begin{itemize}
\item 2 cores operating at 2.8GHz.\todo{Fix these specs}
\item 2 x 32KB write-back L1 data cache. Shared 6MB non-inclusive 24-way set associative L2 cache.
\item 256 entries data, 4-way set associative data TLB.
\end{itemize}
The main memory size of the machine is 1 Gb, and was running Ubuntu
Linux 12.04\todo{Is this correct?}. All implementations have been
written in \texttt{C++11} and compiled on both Linux and Mac OS X
using the clang 3.2 compiler. When measurements was performed, the
code was compiled using the \texttt{-O3 -flto -funroll-loops -march=core2}\todo{Check this}
optimization flags.

In the beginning we ran our tests 3 times. However, it turned out the
variance of our measurements was very small, and to make it feasible
to run our tests on bigger inputs, we adjusted our tests to only
consist of one trial.

\subsection{File structure}
The following is a description of the different folders handed in with
the report.
\begin{description}
\item[code] Contains the implementations of the different streams and
  the external memory sort.

\item[output] Contains the raw measurements used for the plots in the
  report. The plots themselves are also included in a pdf-format.
\end{description}

\subsubsection{Code structure}
The code has the following source files.
\begin{description}
\item[main.cpp] Driver code for calling test code.
\item[Test.hpp] Test framework for generating inputs and measuring
  running times.
\item[CMakeLists.txt] CMake file for the project specifying
  compilation options.

\item Describe rest!\todo{Do this!}
\end{description}

\section{Streams}
This section will present how we benchmarked the four different
streams, and an analysis of their performance. First each stream will
be treated separately and the best settings for each stream will be
used in a relative performance benchmark for finding the stream most
suitable for sorting.

In order to mimic the behaviour of sorting during the test, we have
tested the streams in respectively reading / writing $n$ elements from
/ to a file. Each test uses $k$ interleaved streams to do this, such
that a new stream is starting at every $\frac{n}{k}$ elements, and
elements are being processed in a round robin fashion of the streams.

All tests of the streams have used $n = 2^{28}$, which corresponds to
2GB of elements. This choice was made to make sure the data was
located on the disk, and still small enough to get some data. The
value of $n$ has not been varied for the stream test, as it is not
expected that streams will behave differently, as long as the data is
big enough to not fit the main memory.

In the merge step in the sorting application, the input streams are
used in a very similar way, the output streams are however only used
in the $k = 1$ case, but we have chosen to do testing of these streams
for varying values of $k$, to be able to get comparable results
between input and output streams.

\subsection{Buffered streams}
This type of stream is implemented using the \texttt{read} and
\texttt{write} POSIX system calls. On top of those, a buffer of size
$B$ is maintained keeping track of the next elements to read or write.

It will be investigated how the choice of buffer size and varying
values of $k$ influences the
performance. Figure~\ref{fig:buffered-input} shows a plot of this for
the input stream.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/buffered_input_low}
  \caption{Running times for the buffered input stream using different
    buffer sizes and varying $k$-values.}
  \label{fig:buffered-input}
\end{figure}

From Figure~\ref{fig:buffered-input} it seems like all the streams
perform about the same for sequential read ($k = 1$), which is the
case where the operating system and disk controller is also easily
able to employ caching on lower levels.

As $k$ gets bigger, Figure~\ref{fig:buffered-input} indicates that
bigger buffers gives better performance, which was expected. However,
for large buffer sizes in combination with large values of $k$, the
input stream performs extremely poorly. The slowness is because the
buffers in this case resides on disk, whereby buffering is worse than
using no buffer at all. Larger choices of buffer size was also tried,
which as expected showed a similar behaviour, and is therefore of no
interest for use in sorting.

From Figure~\ref{fig:buffered-input}, it seems that a buffer size of 2
MB provides the best compromise of running time and memory usage for
the input stream in the sorting application.

For the output stream a similar plot is shown in
Figure~\ref{fig:buffered-output}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{images/buffered_output_low}
  \caption{Running times for the buffered output stream using different
    buffer sizes and varying $k$-values.}
  \label{fig:buffered-output}
\end{figure}

It is expected that the plots in Figure~\ref{fig:buffered-input} and
Figure~\ref{fig:buffered-output} are similar, since the two streams
are much alike. However, it is seen that the varying the buffer size
almost have no effect on the performance of the output streams, except
when they are too big to be in memory\todo{This seems to happen before
  in the write case?}.

Another experiment was performed to explain this behaviour, where the
\texttt{fsync(2)}\footnote{The \texttt{fsync(2)} system call ensures
  that the file is written to disk immediately.} system call was
called immediately after each write operation. It was found that both
the input and output streams performed very similar, hence the absence
of effect in buffer size change showed in
Figure~\ref{fig:buffered-output} can be explained by the operating
system delaying writes to the disk.

A buffer size of 2 MB has been chosen to give the best trade-off
between running time and memory consumption. From
Figure~\ref{fig:buffered-output} it seems like a tiny buffer would be
a better choice, but if the operating system for some reason fails
delay writes in the sorting application, a buffer size of 2 MB
provides more confidence that the stream will actually perform as
expected when used in sorting.

\subsection{MMap streams}

\subsection{Other streams}

\subsection{Relative comparison}
In order to determine which stream to use, it is very important that
the input streams performs well for varying values of $k$. This
however, is not as important for the output stream since it is not
interleaved, but other disk access may happen in between writes to the
output stream in the sorting, meaning that it is also is preferable if
the output stream performs decently for different $k$ values, as this
indicate that the output stream is more robust for interleaved disk
operations.

\section{Sorting}

%\section{Algorithms and data structures}
%\input{algorithms.tex}
%\clearpage{}
%\section{Benchmarks}
%\input{benchmarks.tex}

\section{Conclusion}
\input{conclusion.tex}

\clearpage{}\bibliographystyle{plain}
\addcontentsline{toc}{section}{\refname}\bibliography{ref}

\end{document}

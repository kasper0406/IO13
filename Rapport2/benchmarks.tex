%!TEX root = rapport.tex

\subsection{Sift algorithm variations}
As described in Section~\ref{sec:heap:sifting} two different strategies for sifting have been implemented. A memory efficient using less than $V$ space that reads/writes backwards and a version that uses up to $2V$ space but only reads/writes forwards.

We choose to do a small experiment \todo{More details about the test?} for our fastest streams with the two sifting implementations, as we were unsure about the penalty (if any) obtained by doing I/O backwards.

Doing the tests it was found that the space efficient implementation consistently had a better running time \todo{Decribe how much better?}. Hence, the memory efficient has been chosen for all the following experiments.

\subsection{Parameter weeding}
As described in Section~\ref{sec:implementation:parameters}, the implemented heap has a lot of adjustable parameters, which all may have an influence on its performance. In order to limit the time needed for running experiments, the parameters space has been weeded out, such that it does not include obviously bad settings.

We have only considered settings of parameters satisfying the following constraints:
\begin{equation}
  \label{eqn:memory}
  \underbrace{V}_{\textrm{Sifting}} + \underbrace{2P \ceil*{\frac{N}{V}}}_{\textrm{Caching}} + \underbrace{(d + 1)B}_{\mathclap{\substack{
  \textrm{Refill for}\\\textrm{buffered streams}
  }}} \leq M
\end{equation}
\begin{equation}
  \label{eqn:dvn}
  dV\leq N
\end{equation}
Constraint~\ref{eqn:memory} says that all buffers and caches used by the heap should fit into main memory. If this was not the case, memory trashing would occur, giving more I/Os than anticipated. Moreover parameter values have been increased by a factor of $2$. Note that for memory mapped stream there is no explicit buffer, hence when the inequality in ~\ref{eqn:memory} is tight no space is left for the operating system to cache files opened with memory mapping.

Assume a fixed block size, then the performance of sifting improves when the height $h = \log_d{\frac{N}{V}}$ of the tree is decreased. Because the recursion of sifting ends after fewer iterations. When refilling $d+1$ streams are open, hence when using a buffered stream $d+1$ buffers are allocated. Therefore, $d$ be increased as long as the buffers fit into memory.

All configurations not satisfying constraint~\ref{eqn:dvn} will have the same result as $d=\frac{N}{V}$.

\subsection{Streams}
In this section the different streams will be benchmarked and compared.

\subsubsection{Cached stream}
As expected, we have found that if the cached stream is not used, the number of reads from the hard disk increases dramatically. Hence, a significant speedup is found for all of the streams by using a cached stream in front of them.

Experiments determining the influence of the cache size is postponed until the specific streams are discussed, since a given cache size may not yield consistent results for different streams.

\subsubsection{SysStream and FStream}
By experiments, we have found that SysStream and FStream performs worse for all tested parameter combinations. Therefore, we have chosen to exclude these streams from all subsequent comparisons.

The reason for the SysStream being slow, is due to its lack of buffering to reduce the number of I/Os. For FStream a buffer is added, however, we found that it was dominated by BufferedStream. This is also to be expected, since the buffers used in the BufferedStream are optimized for the specific application of an external heap, whereas this is not the case for the more generic FStream.

\subsection{BufferedStream}

For BufferedStream the memory efficient sifting algorithm was the highest performing. This is believed to be due to the same reasoning as explained in subsection \todo{subsectionen!}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{b_final.pdf}
  \caption{Time for sorting a 1GB file varying the buffer size (x-axis) and the node size (series)}
  \label{fig:b_final}
\end{figure}

As can be seen in figure~\ref{fig:b_final}, the highest performing configuration for buffered stream sorting 1 GB elements is a 32 kB buffer size and a node size of 4 MB. Larger block size forces $d$ to be smaller which is shown in figure \todo{Den figur} is not good for best performance. But 4 MB node size is also faster than 2 MB node size. We believe this is due to the fact that a larger node size yields more sequential read/write that buffered streams are especially suited for. When the node size increases, the best buffer size also increases. Having a larger buffer size means more sequential reads/writes and higher performance when utilized properly.

Another interesting observation from the figure is that the configurations (except for a node size of 32 MB) starts to perform worse at a point when using larger buffer sizes. This effect is especially visible for a node size of 2 MB. We believe this is due to underutilized buffers. For instance, consider a refill using a node size of 2 MB, $d=512$ and a 32 kB buffer. The algorithm potentially refills $\frac{V}{2}=\frac{\textrm{2 MB}}{2}$ elements and assuming an equal number of elements from each child is moved up, $\frac{V}{2d}=\frac{\textrm{2 MB}}{2*512}=\textrm{2 kB}$ elements from each child is used which is significantly less than the buffer size of each child. A similar argument can be used for sifting where a smaller portion is sifted upwards when $d$ is larger and therefore, the buffer is again potentially underutilized.

\todo{back det op med 2 MB via TOUCHED}

From this point on, a node size of 4 MB is used when comparing against other other streams.

\subsection{MMapStream}

% TODO: Find konfigurationer

\subsection{Heap}

% TODO: Med de bedste stream konfigurationer + cached

\subsection{Comparison}